TITANIC / KAGGLE / KERNEL READS
	ORDINAL NUMBERS MEANS 
	AN ORDINAL NUMBER REFERS TO A NUMBER THAT INDICATES THE POSITION OR ORDER OF THINGS OR OBJECTS, SUCH AS FIRST, SECOND, THIRD, FOURTH, AND SO ON. ORDINAL NUMBERS DO NOT INDICATE QUANTITY AS CARDINAL NUMBERS DO	
	CLASSIFYING FEATURES
	FOR EXAMPLE, GENDER IS A CATEGORICAL VARIABLE HAVING TWO CATEGORIES (MALE AND FEMALE). NOW WE CANNOT SORT OR GIVE ANY ORDERING TO SUCH VARIABLES. THEY ARE ALSO KNOWN AS NOMINAL VARIABLES
	CATEGORICAL		-	CATEGORY, ORDINAL
	NUMERIC			-	CONTINOUS, DISCRETE, TIME SERIES
	STEPS:	BEST PRACTICES
	* GET DATASET
	* ANALYSE BY DESCRIBING DATASET
		* WHICH FEATURES ARE CATEGORICAL
		* WHICH FEATURES ARE NUMERICAL
		* WHICH FEATURES ARE MIX DATA TYPE
		* WHICH FEATURES ARE MORE ERROR PRONE
		* DATA TYPES OF DIFFERENT FEATURES
		* WHAT IS THE DISTRIBUTION OF NUMERICAL FEATURE & CATEGORICAL VALUES ACROSS THE SAMPLES ?
			LIKE 75% OF MALES ARE ALIVE
			NEARLY 30% OF PASSENGERS CAME WITH SIBLINGS
			- DATA.DESCRIBE()
	* ASSUMPTIONS BASED ON DATA ANALYSIS - WHICH ALL FEATURES ARE TO CONSIDER FOR EACH OF THE STAGE
		* CORRELATING
		* CORRECTION
		* COMPLETIION
	* ANALYSE BY PIVOTING FEATURES
		LIKE FEMALES HAS MORE SURVIVED THAN MEN, 
		BY TAKING MEAN OF SURVIVED VALUES IN GROUP BY GENDER
	* ANALYSE BY VISUALIZING DATA
		* CORRELATING FEATURES
			HISTOGRAM - NUMERICAL CONTINOUS FEATURES
					  - NUMERICAL ORDINAL FEATURES
			POINT PLOT- CATEGORICAL NUMERICAL FEATURES
			BAR PLOT  - CATEGORICAL NUMERICAL FEATURES
	* COMPLETING NUMERICAL CONTINOUS FEATURE
		* SIMPLE WAY TO GENERATE RANDOM NUMBERS BETWEEN MEAN & STANDARD DEVIATION
		* MORE ACCURATE WAY OF GUESSING MISSING VALUES IS TO USE OTHER CORRELATED FEATURES AND TAKING MEDIAN
		* COMBINE 1 & 2
	* COMPLETING CATEGORICAL FEATURE
		* MOST FREQUENT OCCURENCE
	* MODEL, PREDICT, SOLVE
		* LOGISTIC REGRESSION IS A USEFUL MODEL TO RUN EARLY IN THE WORKFLOW. 
		  LOGISTIC REGRESSION MEASURES THE RELATIONSHIP BETWEEN THE CATEGORICAL 
		  DEPENDENT VARIABLE (FEATURE) AND ONE OR MORE INDEPENDENT VARIABLES 
		  (FEATURES) BY ESTIMATING PROBABILITIES USING A LOGISTIC FUNCTION, 
		  WHICH IS THE CUMULATIVE LOGISTIC DISTRIBUTION.

QUARTILES IN STATISTICS ARE VALUES THAT DIVIDE YOUR DATA INTO QUARTERS

1 2		*		3 4		*		5 6		*		7 8
		|---   INTERQUARTILE RANGE	 ---|

WHY DO WE NEED QUARTILES IN STATISTICS? 
THE MAIN REASON IS TO PERFORM FURTHER CALCULATIONS, LIKE THE INTERQUARTILE RANGE, 
WHICH IS A MEASURE OF HOW THE DATA IS SPREAD OUT AROUND THE MEAN

TO FIND OUT THE RELATIONSHIP BETWEEN TWO VARIABLES
CORRELATION VALUES GIVES YOU THE STRENGTH OF IT IN SINGLE VALUE -1 TO +1

DEPENDENT VARIABLE OR OUTPUT VARIABLE ALSO CALLED AS "GROUND TRUTH"
PLOT.LY PROVIDES FULL INTERACTIVITY GRAPH AND MANY EDITING TOOLS. PLOTLY GRAPHS ARE HOSTED USING AN ONLINE WEB SERVICE, SO FIRST YOU WILL HAVE TO SETUP FREE ACOUNT TO STORE YOUR PLOTS. ONLY IF ONLINE
PLOTLY PLOTS CAN BE STORED IN ONLINE/OFFLINE

TO APPLY ACTION ACROSS ALL ROWS OF PARTICULAR COLUMN
DATAFRAME:

TRAIN['NAME_LENGTH'] = TRAIN['NAME'].APPLY(LEN)
TRAIN['HAS_CABIN'] = TRAIN["CABIN"].APPLY(LAMBDA X: 0 IF TYPE(X) == FLOAT ELSE 1)
DATASET.LOC[DATASET['FAMILYSIZE'] == 1, 'ISALONE'] = 1
WHEREAS LOC METHOD HELPS US TO ACCESS ROWS, RETURN TYPE THINK KIND OF SUBSET DATAFRAME
TYPE(NAN) == FLOAT

REFERENCE: https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python

COMPHREHENSIVE DATA ANALYSIS
UNDERSTAND THE PROBLEM			

WE'LL LOOK AT EACH VARIABLE AND DO A PHILOSOPHICAL ANALYSIS 
ABOUT THEIR MEANING AND IMPORTANCE FOR THIS PROBLEM
	
UNIVARIABLE STUDY				
MULTIVARIATE STUDY
BASIC CLEANING
TESTING
	
DATA VISUALIZATION 
	
RELATIONSHIP WITH NUMERICAL VARIABLES						HISTOGRAM
RELATIONSHIP WITH NUERMICAL & CATEGORICAL VARIABLES			BOXPLOT
	
MULTICOLLINEARITY					

CORRELATION IS VERY STRONG BETWEEN THE DEPENDENT VARIABLES 
WHICH MEANS THEY WILL GIVE ALMOST THE SAME INFORMATION
	
MISSING VALUES	CONSIDERATION: 		

WHEN MORE THAN 15% OF THE DATA IS MISSING, 
WE SHOULD DELETE THE CORRESPONDING VARIABLE AND PRETEND IT NEVER EXISTED

OUTLIERS							

THE PRIMARY CONCERN HERE IS TO ESTABLISH A THRESHOLD THAT DEFINES AN OBSERVATION 
AS AN OUTLIER. TO DO SO, WE'LL STANDARDIZE THE DATA. IN THIS CONTEXT, 
DATA STANDARDIZATION MEANS CONVERTING DATA VALUES TO HAVE MEAN OF 0 AND A STANDARD DEVIATION OF 1.

HOW TO SOLVE MULTICOLLINEARITY ?

IN SMALL DATASET WHERE NUMBER OF FEATURES IS LESS, THERE WE CAN PLOT ALL FEATURES IN CORRELATION HEAT MAP
THROUGH BY WHICH WE GET TO KNOW WHICH FEATURES ARE HIGHLY CORRELATED.
WE CAN DROP ANY OF THE HIGHLY CORRELATED FEATURES
EVEN IN DROPPING FEATURES, THERE IS A PROBLEM OF LOSING INFORMATION
SO TRY TO USE REGULARIZATION IN THOSE PLACES

BUT IN LARGE DATASET WHERE NUMBER OF FEATURES IS LARGE, THERE WE CAN'T SIMPLY DROP THE CORRELATED FEATURE
WE SHOULD USE EITHER RIDGE OR LASSO REGRESSION
ALSO WE KNOW THAT REGULARIZATION WILL PENALISE THE IRRELAVANT FEATURES FROM MODEL BY LAMBDA VALUES

INDEPENDENCE OF ERROR

IF YOUR POINTS ARE FOLLOWING A CLEAR PATTERN, IT MIGHT INDICATE THAT THE ERRORS ARE INFLUENCING EACH OTHER. 
THE ERRORS ARE THE DEVIATIONS OF AN OBSERVED VALUE FROM THE TRUE FUNCTION VALUE. 
IF YOU DON’T HAVE RANDOM ERRORS, YOU CAN’T RUN LINEAR REGRESSION AS YOUR PREDICTIONS WON’T BE ACCURATE.

DECISION TREES
	.....HOW TO DECIDE WHICH FEATURE SHOULD CONSIDER FIRST FOR BUILDING IT TREES ? 
	FIRST WE SHOULD CONSIDER EACH FEATURE TO COMPARE WITH OUTPUT VARIABLE DIRECTLY
	THEN WE SHOULD CHECK WHETHER THIS FEATURES HELPS IN PREDICTING OUTPUT DIRECTLY OR NOT ?
		IF YES, THEN WE DONT NEED TO PROCEED FURTHER ON
		IF NOT, THEN IT IS CALLED IMPURE STATE. COMPUTE THE IMPURITY LEVEL OF ITS FEATURE
		MOST POPULAR WAY: GINI MEASUREMENT
		WHICH IS NOTHING BUT
								FEATURE A
						
			VALUE, IF TRUE CASE					IF FALSE CASE
				
			HEART DISEASE						HEART DISEASE
			YES 90	NO 50						YES 40	NO 120			THIS IS CALLED IMPURE, SINCE FINAL OUTPUT NOT 
																		VERY HELPFUL IN PREDICTING OUTPUT CLEARLY & DIRECTLY

		GINI IMPURITY OF FEATURE A TRUE CASE  IS (1 - SQUARE(PROBABILITY OF "YES") - SQUARE(PROBABILITY OF "NO"))
											  IS (1 - SQUARE(90/(90 + 50)) - SQUARE(50/ (90 + 50)))
											  IS (VALUE A)
		GINI IMPURITY OF FEATURE A FALSE CASE IS (1 - SQUARE(PROBABILITY OF "YES") - SQUARE(PROBABILITY OF "NO"))
											  IS (1 - SQUARE(40/(40 + 120)) - SQUARE(120/ (40 + 120)))
											  IS (VALUE B)

		HERE 140 IS TOTAL NUMBER OF LEAF NODES IN TRUE CASE OF FEATURE A
		WHEREAS 160 IS TOTAL NUMBER OF LEAF NODES IN FALSE CASE OF FEATURE A
		TOTAL GINI IMPURITY OF FEATURE A 	  IS ((140/(140+160)) * VALUE A) + ((160/(140+160)) * VALUE B)
		
		FINALLY WHICHEVER FEATURE IS HAVING LOW IMPURITY LEVEL AMONG THOSE, THAT NEEDS TO CONSIDER FIRST.
	THEN WHILE CONSIDERING FOR NEXT INTERNAL NODES, FIRST WE SHOULD CHECK ALL THESE
		IF NODE ITSELF HAS LOWEST IMPURITY SCORE, THEN THERE IS NOT POINT IN SEPERATING THE RESULT ANY MORE
		AND IT BECOMES A LEAF
	THIS PROCESS CONTINUES, UNTIL ALL THE FEATURES ARE USED IN DECISION TREES.
	THIS IS HOW THE ALGORITHM WORKS IN BUILDING DECISION TREES.
	
	NOTE: ABOVE METHOD ONLY WORKS, 
	IF FEATURE = CATEGORICAL VALUE THEN 
		PRINT("NO NEED OF ANY PRECOMPUTATION")
	ELSE IF FEATURE = NUMERIC THEN 
		FOLLOW THESE STEPS ON BEFORE:
		SORT THE VALUES IN ASCENDING ORDER
		AVERAGE THE VALUE WITH THE ADJACENT NODE
	ELSE IF FEATURE = ORDINAL THEN # SAY FOR EG: RANK 1, RANK 2, RANK 3, RANK 4
		# HINT: TRY OUT THE EACH VALUE 
		# <= RANK 1 OR.... BUT NOT <= RANK 4 SINCE IT INCLUDES EVERYTHING AGAIN
	ELSE IF FEATURE = NOMINAL THEN # SAY FOR EG: RED, BLUE, GREEN
		# HINT: TRY OUT THE EACH COMBINATION
		# RED, BLUE, GREEN, RED OR BLUE, RED OR GREE,... BUT NOT RED OR BLUE OR GREEN SINCE IT INCLUDES EVERYTHING AGAIN
	THEN FOLLOW THE ABOVE METHOD ITSELF...
	.....REGRESSION TREES ARE A TYPE OF DECISION TREES, HERE EACH LEAF VALUE REPRESENTS A NUMERIC VALUE
	BUT IN CONTRAST, CLASSIFICATION TREES GIVES CATEGORICAL VALUE
RANDOM FOREST
	FIRST BOOTSTRAP THE DATASET BY SELECTING SAMPLES RANDOMLY (ALLOW DUPLICATE ENTRIES AS WELL)
	THEN RANDOMLY PICK THE SUBSET OF FEATURES FOR BUILDING TREE
	REPEAT ABOVE PROCESS N TIMES
	
	BAGGING: BOOTSTRAPING THE DATA PLUS USING THE AGGREGATE TO MAKE A DECISION
	ALSO WHILE BOOTSTRAP DATASET, ENTRY WHICH DOESN'T GET ADDED UP IS CALLED OUT OF BAG SAMPLE
	ULTIMATELY WE CAN MEASURE THE ACCURACY OF OUR RANDOM FOREST THROUGH BY THE PROPORTION OF OUT OF BAG SAMPLES THAT
	WERE CORRECTLY CLASSIFIED BY THE RANDOM FOREST
	
	STEPS:
	BUILD A RANDOM FOREST
	ESTIMATE THE ACCURACY OF RANDOM FOREST - OUT OF BAG TEST
	IF NOT GGOD, THEN CHANGE THE NUMBER OF FEATURES USED PER STEP
	DO BUNCH OF TIMES
		TYPICALLY WE START BY USING THE SQUARE OF NUMBER OF VARIABLES AND THEN TRY A FEW SETTINGS ABOVE & BELOW THAT VALUE
		
	FILLED IN MISSING VALUES:
	INITIALLY REPLACE THE MISSING VALUE WITH MOST OCCURENCE FOR CATEGORICAL &
	FOR NUMERIC, REPLACE IT WITH MEDIAN.
	THEN RUN ALL OF THE DATA DOWN ALL OF THE TREES
	WE KEEP TRACK OF SIMILAR SAMPLES USING A PROXIMITY MATRIX (SAMPLES, WHICH ENDS UP AT THE SAME LEAF NODE ON SAME TREE)
	PROXIMITY MATRIX HAS A ROW & COLUMN FOR EACH SAMPLE, ASSUME PROXIMITY MATRIX HAS 0 IN ALL OF THE CELLS
	ATLAST NOW WE USE THE PROXIMITY VALUES FOR BETTER GUESS OF MISSING VALUES WITH MOST SIMILIAR SAMPLES
	
	PROXIMITY MATRIX
	
					SAMPLE 1	SAMPLE 2	SAMPLE 3	SAMPLE 4
		
		SAMPLE 1	0/10		2/10		1/10		1/10
		
		SAMPLE 2	2/10		0/10		1/10		1/10			
		
		SAMPLE 3	1/10		1/10		0/10		8/10
		
		SAMPLE 4	1/10		1/10		8/10		0/10
		
	FROM THIS, SAMPLE 4 IS MORE SAME AS SAMPLE 3 = 80% SIMILARITY
	FOR COMPUTING ANY MISSING VALUE IN SAMPLE 4 IS
		
		HAS BLOCKED ARTERIES	
	PROBABILITY OF BEING YES	=	PROBABILITY(YES)/TOTAL PROBABILITY * (PROXIMITY VALUE OF BEING YES)/TOTAL PROXIMITY VALUE
								=	1/3 * 0.1/(0.1+0.1+0.8)
								
	PROBABILITY OF BEING NO		=	PROBABILITY(NO)/TOTAL PROBABILITY * (PROXIMITY VALUE OF BEING NO)/TOTAL PROXIMITY VALUE
								=	2/3 * 0.9/(0.1+0.1+0.8)
								
	BUILD THE RANDOM FOREST
	RUN THE DATA THROUGHOUT THE TREES
	RECALCULATE THE PROXIMITIES
	RECALCULATE THE MISSING VALUES
	REPEAT STEPS UNTIL MISSING VALUES CONVERGE TO SOME STABLE VALUES
	
	PROXIMITY MATRIX ALSO VERY HELPFUL IN GIVING DISTANCE BETWEEN TWO SAMPLES = 1 - PROXIMITY VALUE(SAMPLE)
	HERE VALUES CLOSE TO 0 SHOWS HOW BOTH SAMPLES WERE RELATED TO EACH OTHER
EXTRA TREE CLASSIFIER
	IS A TYPE OF ENSEMBLING LEARNING TECHNIQUE WHICH AGGREGATES THE RESULT OF MULTIPLE DE-CORRELATED DECISION TREES COLLECTED
	IN A FOREST TO OUTPUT ITS CLASSIFICATION RESULT. IN CONCEPT IT IS VERY SIMILIAR TO RANDOM FOREST AND ONLY DIFFERS FROM
	IT IN THE MANNER OF CONSTRUCTION OF DECISION TREES IN THE FOREST
APRIL 20, 2020		10:20 PM
DATA EXPLORATION COMPHREHENSIVE GUIDE
	VARIABLE IDENTIFICATION			VARIABLE TYPE, DATA TYPE, VARIABLE CATEGORY
	UNIVARIATE ANALYSIS				VERY HELPFUL IN HIGHLIGHTING MISSING VALUES AND OUTLIER DETECTION
									WHILE DOING THIS ANALYSIS, MORE PREFERABLE CHART TYPE IS BOX PLOT & HISTOGRAM
	BIVARIABLE ANALYSIS				HELPS IN FIGURING OUT RELATIONSHIP BETWEEN VARIABLES
									WHILE DOING THIS ANALYSIS
										FOR TWO CONTINOUS VARIABLES   - USE SCATTER PLOT
																		SCATTER PLOT SHOWS THE RELATIONSHIP BETWEEN 
																		TWO VARIABLES BUT IT DOES NOT INDICATE THE STRENGTH 
																		OF RELATIONSHIP AMONGST THEM. 
																		FOR STRENGTH OF RELATIONSHIP, USE CORRELATION
										FOR TWO CATEGORICAL VARIABLES - USE TABLE WITH HAVING METRICS AS COUNT & COUNT %, 
																		STACK COLUMN CHART
										FOR CATEGORICAL & CONTINOUS   - USE GROUP BOX PLOT IF CATEGORIES ARE MORE
																		ELSE USE Z TEST, T TEST, ANOVA
																		GENERALLY Z TEST OR T TEST WILL ASSESS MEAN OF TWO
																		GROUPS ARE STATISTICALLY DIFFERENT OR NOT
																
	MISSING VALUES TREATMENT		
	OUTLIER TREATMENT				GENERALLY OUTLIERS WILL BE DETECTED EITHER IN UNIVARIATE OR MULTIVARIATE ANALYSIS
									MOST COMMON VISUALIZATION FOR FINDING OUTLIERS ARE
										BOX PLOT
										HISTOGRAM
										SCATTER PLOT
									ALSO SOME GENERAL THUMBRULE ARE
										ANY VALUE, WHICH IS BEYOND THE RANGE OF -1.5 X IQR TO 1.5 X IQR
										ANY VALUE WHICH OUT OF RANGE OF 5TH AND 95TH PERCENTILE
										DATA POINTS, THREE OR MORE STANDARD DEVIATION AWAY FROM MEAN                    
	FEATURE ENGINEERING 			VARIABLE TRANSFORMATION
										ONE QUICK EXAMPLE OF TRANSFORMATION IS:
											FOR RIGHT SKEW DISTRIBUTION, TAKE SQUARE, CUBE, LOGARITHM OF VARIABLE
											FOR LEFT SKEW DISTRIBUTION, TAKE SQUARE, CUBE, EXPONENTIATION
											IN CASE OF POSITIVE SKEWNESS, LOG TRANSFORMATIONS USUALLY WORKS WELL
									VARIABLE CREATION
GETTING HARDCORE
	NORMALITY						THE DATA SHOULD LOOK LIKE A NORMAL DISTRIBUTION. THIS IS IMPORTANT BECAUSE SEVERAL STATISTIC 
									TESTS RELY ON THIS (E.G. T-STATISTICS). 
									IN THIS EXERCISE WE'LL JUST CHECK UNIVARIATE NORMALITY FOR 'SALEPRICE'
									(WHICH IS A LIMITED APPROACH). REMEMBER THAT UNIVARIATE NORMALITY DOESN'T ENSURE 
									MULTIVARIATE NORMALITY (WHICH IS WHAT WE WOULD LIKE TO HAVE), BUT IT HELPS. 
									ANOTHER DETAIL TO TAKE INTO ACCOUNT IS THAT IN BIG SAMPLES (>200 OBSERVATIONS) 
									NORMALITY IS NOT SUCH AN ISSUE. HOWEVER, IF WE SOLVE NORMALITY, WE AVOID A LOT OF 
									OTHER PROBLEMS (E.G. HETEROSCEDACITY, HOMOSCEDASTICITY)
	HOMOSCEDASTICITY				HOMO MEANS SAME, SCEDASTICITY MEANS SCATTERING
									EG:
										SURVEY OF 1000 FAMILIES OF FATHER'S & DAUGHTER'S AGE WERE PLOTTED IN SCATTER PLOT
										ASSUME THESE RELATIONS WERE PERSIST IN THAT SCATTER PLOT
											
										1ST RANDOM DATA POINT, WHERE FATHER'S AGE IS 60, THEN THEIR DAUGHTER'S AGE IS 
										AROUND BETWEEN 25 - 35, 2ND RANDOM DATA POINT WHERE FATHER'S AGE IS 70, 
										THEN THEIR DAUGHTER'S AGE IS BTW 35 - 45
										
										ABOVE RELATION SHOWS THAT,
										SCATTERING FOLLOWS THE SAME VARIANCE OF DAUGHTER'S AGE THAT IS BETWEEN 10 AGES		
										TO BE MORE PRECISE, THIS VARIANCE IS VERY CLOSELY RELATED WITH PREDICTOR ERROR.
										HOMOSCEDASTICITY IS NOTHING BUT SPREAD OF THE DATA IS SAME EVERYWHERE
	LINEARITY
	ABSENCE OF CORRELATED ERRORS
APRIL 22, 2020		11:58 PM	
PARAMETER							A MODEL PARAMETER IS A CONFIGURATION VARIABLE THAT IS INTERNAL TO THE MODEL AND 
									WHOSE VALUE CAN BE ESTIMATED FROM THE GIVEN DATA.

									THEY ARE REQUIRED BY THE MODEL WHEN MAKING PREDICTIONS.
									THEIR VALUES DEFINE THE SKILL OF THE MODEL ON YOUR PROBLEM.
									THEY ARE ESTIMATED OR LEARNED FROM DATA.
									THEY ARE OFTEN NOT SET MANUALLY BY THE PRACTITIONER.
									THEY ARE OFTEN SAVED AS PART OF THE LEARNED MODEL.
									
HYPERPARAMETER						A MODEL HYPERPARAMETER IS A CONFIGURATION THAT IS EXTERNAL TO THE MODEL AND 
									WHOSE VALUE CANNOT BE ESTIMATED FROM DATA.

									THEY ARE OFTEN USED IN PROCESSES TO HELP ESTIMATE MODEL PARAMETERS.
									THEY ARE OFTEN SPECIFIED BY THE PRACTITIONER.
									THEY CAN OFTEN BE SET USING HEURISTICS.
									THEY ARE OFTEN TUNED FOR A GIVEN PREDICTIVE MODELING PROBLEM.
									
HYPERPARAMETER TUNING / OPTIMIZATION
SIMPLE WAYS
GRID SEARCH & RANDOM SEARCH			GRID SEARCH IS NOTHING BUT YOU USE ARRAY OF VALUES FOR EACH OF THE HYPERPARAMETER.
									FROM THAT, IT WILL PICK UP THE BEST COMBINATION OF HYPERPARAMETERS.
									IT WILL BE GENERALLY USED FOR PROBLEMS WITH LOW DIMENSION.
									
									RANDOM SEARCH IS GENERALLY MORE EFFICIENT THAN GRID SEARCH FOR PROBLEMS WITH HIGH DIMENSION
									BUT SOME PROBLEMS LIKE DEEP NEURAL NETWORK WILL TAKE DAYS TO COMPLETE, ON THOSE
									CASES, BAYESIAN OPTIMIZATION IS PREFERRED THAN ANY TECHNIQUE