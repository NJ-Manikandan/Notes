Linear Algebra,
It is a language to describe space and manipulation of space using numbers. Generally sequence of operations in linear algebra would happen from right to left.

Basis vector,
Always in linear algebra we will start with these basis vectors implicitly.
images: 1-1

Three distinct ideas about vector, In Physics students prespective, vectors are arrows pointing in space what defines a vector is, length and direction it is pointing in. In computer science students prespective, vectors are ordered list of numbers. Mathematician on the other hand, a vector can be anything where there is a sensible notion of adding two vectors and multiplying a vector by number.
images: 1-2

Vector addition,
Think about a vector representing a certain movement in space of specified length and direction. Oru edathula irundhu inoru edathuku pora madhiri.
images: 2-1, 2-2, 2-3

Scalar,
Number by which it is expanding or squishing the vector is called scalar. The process of expanding or squishing a vector is called scaling. 
images: 3

2-Dimensional span,
Set of all possible vectors that you can reach with a linear combinations of a given pairs of vectors is called their span. images: 4-1, 4-2, 4-3, 4-4, 4-5
In case those initial pair of vector pointing in same direction, Then their span is just a line. Linearly dependent vectors.
images: 5-1

Matrice,
Matrices by itself is a pack of more than one vector.

Matrices as linear transformation,
Linear transformation are a way to move around space such that grid lines remain parallel and evenly spaced and such that origin remains fixed. so every time you see a matrix you can interpret it as certain transformation of space. Applying matrice to any vector causes gradual movement from one place to another.
images: 6-1, 6-2, 6-3, 6-4, 6-5
If columns in matrices are linearly dependent. Then it squishes the entire space to one dimensional line.
images: 7-1, 7-2 

Properties of matrice multiplication,
images: 8-1

Determinant,
Factor by which a linear transformation changes any area is called determinant of the transformation. whenever the orientation of space got flipped over by linear transformation then determinant will be negative.
images: 9-1

Linear system of equations,
images: 10-1

Inverse matrice,
As long as transformation doesn't squish space into a lower dimension meaning its determinant is non zero, then the inverse exist. whereas in other case if it squishes space then there is no inverse. its still possible solution can even exist when there is no inverse, you have to be lucky enough that the expected output vector lies somewhere on the shrinked space itself. While linear transformation there will be whole bunch of vectors from different direction landing onto that output vector. 
images: 11-1

Column space,
After linear transformation, span of its column is called column space.
images: 12-1

Rank of matrice,
After linear transformation, number of dimensions in the column space is called rank.
images: 13-1

Null space or kernel of matrice,
Notice zero vector will always be included in the column space. since linear transformations must keep the origin fixed in place. For a full rank transformation, the only vector that lands at the origin is the zero vector. But for matrices that aren't full rank, Which squish space into smaller dimension you can have whole bunch of vectors that land at the origin. If a 2D transformation squishes space onto a line for example there is a seperate line in different direction full of vectors that gets squished onto the origin. This set of vectors that land on the origin is called null space or kernel of matrice.
images: 14-1, 14-2, 14-3, 14-4

Non square matrices,
Generally number of columns in matrice specifies the input space, Number of rows specifies the output space. Following matrice is still a full rank, since number of dimensions in input space and output space is still same.
images: 15-1

Dot product,
images: 16-1, 16-2, 16-3

images: 16-4
Here order doesn't matter. You could instead project v onto w, multiply the length of the projected v by the length of w and get the same result.

why dot product have anything to do with projection?
What I'm going to do here is take a copy of the number line and place it diagonally and space somehow with the number 0 sitting at the origin.
images: 16-5

But that vector u-hat is a two-dimensional vector living in the input space. It's just situated in such a way that overlaps with the embedding of the number line. With this projection, we just defined a linear transformation from 2D vectors to numbers,
so we're able to find some kind of 1 x 2 matrix that describes the transformation and where the basis vector lands.

since i-hat and u-hat are both unit vectors, projecting i-hat onto the line passing through u-hat looks totally symmetric to protecting u-hat onto the x-axis. So when we asked what number does i-hat land on when it gets projected the answer is going to be the same as whatever u-hat lands on when its projected onto the x-axis but projecting u-hat onto the x-axis just means taking the x-coordinate of u-hat. So, by symmetry, the number where i-hat lands when it’s projected onto that diagonal number line is going to be the x coordinate of u-hat. The reasoning is almost identical for the j-hat case.
images: 16-6, 16-7, 16-8

So the entries of the 1 x 2 matrix describing the projection transformation are going to be the coordinates of u-hat.
images: 16-9

And computing this projection transformation for arbitrary vectors in space, which requires multiplying that matrix by those vectors, is computationally identical to taking a dot product with u-hat. This is why taking the dot product with a unit vector,
can be interpreted as projecting a vector onto the span of that unit vector and taking the length.
images: 16-10, 16-11

So what about non-unit vectors? For example, let's say we take that unit vector u-hat, but we “scale” it up by a factor of 3.
Numerically, each of its components gets multiplied by 3, So looking at the matrix associated with that vector, it takes i-hat and j-hat to 3 times the values where they landed before. Since this is all linear, it implies more generally, that the new matrix can be interpreted as projecting any vector onto the number line copy and multiplying where it lands by 3. This is why the dot product with a non-unit vector can be interpreted as first projecting onto that vector then scaling up the length of that projection by the length of the vector.
images: 16-12, 16-13

The lesson here, is that anytime you have one of these linear transformations from any space to the number line, Its associated with a unique vector in that space, In the sense performing the linear transformation is same as taking a dot product with that unique vector. It's an example of something in math called “duality”. That unique vector is called dual vector.
images: 16-14

Cross product,
In 2-D, cross-product is just the determinant. But the true cross product is something that combines two different 3D vectors to get a new 3D vector.
images: 17-1, 17-2

This new vector's length will be the area of the parallelogram. And the direction of that new vector is going to be perpendicular to the parallelogram with following the right hand rule.
images: 17-3, 17-4

As we all know area of the parallelogram is nothing but Length X Breadth. Now try to convert parallelopiped to rectangle then you will get L X B X H. Here the height will be identical to projected length of vector X to the line which is perpendicular to the parallelogram. Finally taking dot product between P and V vector is Length of vector P (which is assumed L X B) X projected length of vector X (H).
images: 17-5, 17-6, 17-7, 17-8, 17-9

Change of basis vector,
Jennifer basis vector is represented little differently
images: 18-1
Conversion of one basis vector to another basis vector
images: 18-2, 18-3, 18-4, 18-5

Eigen vector,
Many vectors will get knocked off their span during the transformation. some special vectors do remain on their own span meaning the effect that, the matrix has on those vectors is just to stretch it or squish it like a scalar. those are called eigen vectors and the factor by which it stretches or squishes is called eigen value. When it is useful ? Consider some 3D rotation, If you can find an eigen vector for a rotation, what you have found is axis of rotation. Most interestingly if you have any diagonal matrix, then all their basis vectors are eigen vectors and diagonal values are eigen values. If you have any other matrix A with more than one eigen vector, Then try using that eigen vectors as your initial basis vectors in your co ordinate system and then do the matrix A transformation, this is guaranteed to result in a diagonal matrix. So generally doing any computation on the diagonal matrix is very simple.
images: 19-1, 19-2, 19-3, 19-4, 19-5, 19-6, 19-7, 19-8

Conclusion,
Point i want to make here is that there are lot of vectorish things in maths, as long as you are dealing with set of objects where there is sensible notion of adding and scaling. anybody who wants to create their own vectorish thing then make sure to check whether all the 8 axioms are getting satisfied. axioms are generally an interface between you as a mathematician and other mathematicians to share and freely explore with their own vectorish thing.
images: 20-1

Notations,
R superscript n, means N dimensional co-ordinate space.

Jacobian matrix,
Jacobian matrix is the matrix representing the best approximated impact of linear transformation to nearby points. 
Informal defintion is, it says more about what it does to nearer points. How it is scaled after linear transformation. 
images: 21-1

Jacobian determinant,
Determinant of jacobian matrix is called jacobian determinant.

Row Echelon form,
The basic idea is to clear variables in successive equations and form an upper triangular matrix